{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras to Build and Train Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise we will use a neural network to predict diabetes using the Pima Diabetes Dataset.  We will start by training a Random Forest to get a performance baseline.  Then we will use the Keras package to quickly build and train a neural network and compare the performance.  We will see how different network structures affect the performance, training time, and level of overfitting (or underfitting).\n",
    "\n",
    "## UCI Pima Diabetes Dataset\n",
    "\n",
    "* UCI ML Repositiory (http://archive.ics.uci.edu/ml/datasets/Pima+Indians+Diabetes)\n",
    "\n",
    "\n",
    "### Attributes: (all numeric-valued)\n",
    "   1. Number of times pregnant\n",
    "   2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "   3. Diastolic blood pressure (mm Hg)\n",
    "   4. Triceps skin fold thickness (mm)\n",
    "   5. 2-Hour serum insulin (mu U/ml)\n",
    "   6. Body mass index (weight in kg/(height in m)^2)\n",
    "   7. Diabetes pedigree function\n",
    "   8. Age (years)\n",
    "   9. Class variable (0 or 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Pima Diabetes Dataset which has 8 numerical predictors and a binary outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preliminaries\n",
    "\n",
    "from __future__ import absolute_import, division, print_function  # Python 2/3 compatibility\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_curve, roc_auc_score, roc_curve, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-21-85407ea78bde>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-21-85407ea78bde>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    python -c 'import tensorflow as tf; print(tf.__version__)'\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Import Keras objects for Deep Learning\n",
    "\n",
    "from keras.models  import Sequential\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, SGD, RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the data set (Internet Access needed)\n",
    "\n",
    "##url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = [\"times_pregnant\", \"glucose_tolerance_test\", \"blood_pressure\", \"skin_thickness\", \"insulin\", \n",
    "         \"bmi\", \"pedigree_function\", \"age\", \"has_diabetes\"]\n",
    "diabetes_df = pd.read_csv('pima-indians-diabetes.data', names=names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>times_pregnant</th>\n",
       "      <th>glucose_tolerance_test</th>\n",
       "      <th>blood_pressure</th>\n",
       "      <th>skin_thickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree_function</th>\n",
       "      <th>age</th>\n",
       "      <th>has_diabetes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32.7</td>\n",
       "      <td>0.734</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>90</td>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "      <td>25.2</td>\n",
       "      <td>0.313</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>12</td>\n",
       "      <td>92</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "      <td>258</td>\n",
       "      <td>27.6</td>\n",
       "      <td>0.926</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>4</td>\n",
       "      <td>99</td>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0.294</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>695</th>\n",
       "      <td>7</td>\n",
       "      <td>142</td>\n",
       "      <td>90</td>\n",
       "      <td>24</td>\n",
       "      <td>480</td>\n",
       "      <td>30.4</td>\n",
       "      <td>0.128</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     times_pregnant  glucose_tolerance_test  blood_pressure  skin_thickness  \\\n",
       "743               9                     140              94               0   \n",
       "500               2                     117              90              19   \n",
       "254              12                      92              62               7   \n",
       "488               4                      99              72              17   \n",
       "695               7                     142              90              24   \n",
       "\n",
       "     insulin   bmi  pedigree_function  age  has_diabetes  \n",
       "743        0  32.7              0.734   45             1  \n",
       "500       71  25.2              0.313   21             0  \n",
       "254      258  27.6              0.926   44             1  \n",
       "488        0  25.6              0.294   28             0  \n",
       "695      480  30.4              0.128   43             1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peek at the data -- if there are lots of \"NaN\" you may have internet connectivity issues\n",
    "print(diabetes_df.shape)\n",
    "diabetes_df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values\n",
    "y = diabetes_df[\"has_diabetes\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data to Train, and Test (75%, 25%)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=11111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3489583333333333, 0.6510416666666666)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y), np.mean(1-y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we see that about 35% of the patients in this dataset have diabetes, while 65% do not.  This means we can get an accuracy of 65% without any model - just declare that no one has diabetes. We will calculate the ROC-AUC score to evaluate performance of our model, and also look at the accuracy as well to see if we improved upon the 65% accuracy.\n",
    "## Exercise: Get a baseline performance using Random Forest\n",
    "To begin, and get a baseline for classifier performance:\n",
    "1. Train a Random Forest model with 200 trees on the training data.\n",
    "2. Calculate the accuracy and roc_auc_score of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=200)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Train the RF Model\n",
    "rf_model = RandomForestClassifier(n_estimators=200)\n",
    "rf_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.776\n",
      "roc-auc is 0.841\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set - both \"hard\" predictions, and the scores (percent of trees voting yes)\n",
    "y_pred_class_rf = rf_model.predict(X_test)\n",
    "y_pred_prob_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "#O método \"predict_classes\" não funciona nas versões do Keras superior a 2.5 \n",
    "\n",
    "# A indicação da correção se encontra em https://keras.rstudio.com/reference/predict_proba.html#details. \n",
    "\n",
    "# usar: \n",
    "# y_pred_class_nn_1 = model_1.predict(X_test_norm)\n",
    "# y_pred_class_nn_1 = (y_pred_prob_nn_1 > 0.5).astype(\"int32\")\n",
    "\n",
    "\n",
    "\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_rf)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_rf[:,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_roc(y_test, y_pred, model_name):\n",
    "    fpr, tpr, thr = roc_curve(y_test, y_pred)\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.plot(fpr, tpr, 'k-')\n",
    "    ax.plot([0, 1], [0, 1], 'k--', linewidth=.5)  # roc curve for random model\n",
    "    ax.grid(True)\n",
    "    ax.set(title='ROC Curve for {} on PIMA diabetes problem'.format(model_name),\n",
    "           xlim=[-0.01, 1.01], ylim=[-0.01, 1.01])\n",
    "\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_rf[:, 1], 'RF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Single Hidden Layer Neural Network\n",
    "\n",
    "We will use the Sequential model to quickly build a neural network.  Our first network will be a single layer network.  We have 8 variables, so we set the input shape to 8.  Let's start by having a single hidden layer with 12 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First let's normalize the data\n",
    "## This aids the training of neural nets by providing numerical stability\n",
    "## Random Forest does not need this as it finds a split only, as opposed to performing matrix multiplications\n",
    "\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train_norm = normalizer.fit_transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Model \n",
    "# Input size is 8-dimensional\n",
    "# 1 hidden layer, 12 hidden nodes, sigmoid activation\n",
    "# Final layer has just one node with a sigmoid activation (standard for binary classification)\n",
    "\n",
    "model_1 = Sequential([\n",
    "    Dense(12, input_shape=(8,), activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 13        \n",
      "=================================================================\n",
      "Total params: 121\n",
      "Trainable params: 121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#  This is a nice tool to view the model you have created and count the parameters\n",
    "\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehension question:\n",
    "Why do we have 121 parameters?  Does that make sense?\n",
    "\n",
    "\n",
    "Let's fit our model for 200 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "18/18 [==============================] - 0s 7ms/step - loss: 0.7919 - accuracy: 0.4410 - val_loss: 0.8090 - val_accuracy: 0.4375\n",
      "Epoch 2/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7846 - accuracy: 0.4583 - val_loss: 0.8019 - val_accuracy: 0.4479\n",
      "Epoch 3/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7776 - accuracy: 0.4670 - val_loss: 0.7952 - val_accuracy: 0.4583\n",
      "Epoch 4/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7708 - accuracy: 0.4757 - val_loss: 0.7887 - val_accuracy: 0.4583\n",
      "Epoch 5/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7644 - accuracy: 0.4774 - val_loss: 0.7824 - val_accuracy: 0.4583\n",
      "Epoch 6/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7583 - accuracy: 0.4809 - val_loss: 0.7764 - val_accuracy: 0.4635\n",
      "Epoch 7/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7524 - accuracy: 0.4948 - val_loss: 0.7706 - val_accuracy: 0.4792\n",
      "Epoch 8/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7467 - accuracy: 0.5122 - val_loss: 0.7651 - val_accuracy: 0.4844\n",
      "Epoch 9/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7413 - accuracy: 0.5174 - val_loss: 0.7596 - val_accuracy: 0.4896\n",
      "Epoch 10/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7360 - accuracy: 0.5243 - val_loss: 0.7544 - val_accuracy: 0.4948\n",
      "Epoch 11/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7309 - accuracy: 0.5295 - val_loss: 0.7494 - val_accuracy: 0.5000\n",
      "Epoch 12/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7260 - accuracy: 0.5399 - val_loss: 0.7445 - val_accuracy: 0.5104\n",
      "Epoch 13/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7213 - accuracy: 0.5556 - val_loss: 0.7398 - val_accuracy: 0.5208\n",
      "Epoch 14/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7168 - accuracy: 0.5625 - val_loss: 0.7352 - val_accuracy: 0.5365\n",
      "Epoch 15/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7124 - accuracy: 0.5729 - val_loss: 0.7308 - val_accuracy: 0.5417\n",
      "Epoch 16/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7081 - accuracy: 0.5920 - val_loss: 0.7264 - val_accuracy: 0.5417\n",
      "Epoch 17/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7040 - accuracy: 0.5938 - val_loss: 0.7222 - val_accuracy: 0.5573\n",
      "Epoch 18/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.7000 - accuracy: 0.6059 - val_loss: 0.7181 - val_accuracy: 0.5625\n",
      "Epoch 19/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6962 - accuracy: 0.6111 - val_loss: 0.7141 - val_accuracy: 0.5625\n",
      "Epoch 20/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6924 - accuracy: 0.6181 - val_loss: 0.7102 - val_accuracy: 0.5625\n",
      "Epoch 21/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.6888 - accuracy: 0.6198 - val_loss: 0.7064 - val_accuracy: 0.5677\n",
      "Epoch 22/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.6198 - val_loss: 0.7027 - val_accuracy: 0.5729\n",
      "Epoch 23/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6817 - accuracy: 0.6233 - val_loss: 0.6990 - val_accuracy: 0.5781\n",
      "Epoch 24/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6784 - accuracy: 0.6267 - val_loss: 0.6955 - val_accuracy: 0.5938\n",
      "Epoch 25/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6750 - accuracy: 0.6337 - val_loss: 0.6920 - val_accuracy: 0.6042\n",
      "Epoch 26/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6717 - accuracy: 0.6406 - val_loss: 0.6886 - val_accuracy: 0.6094\n",
      "Epoch 27/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6685 - accuracy: 0.6406 - val_loss: 0.6852 - val_accuracy: 0.6146\n",
      "Epoch 28/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6654 - accuracy: 0.6458 - val_loss: 0.6818 - val_accuracy: 0.6250\n",
      "Epoch 29/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6622 - accuracy: 0.6528 - val_loss: 0.6786 - val_accuracy: 0.6250\n",
      "Epoch 30/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6592 - accuracy: 0.6562 - val_loss: 0.6754 - val_accuracy: 0.6302\n",
      "Epoch 31/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6562 - accuracy: 0.6580 - val_loss: 0.6722 - val_accuracy: 0.6354\n",
      "Epoch 32/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6533 - accuracy: 0.6597 - val_loss: 0.6692 - val_accuracy: 0.6406\n",
      "Epoch 33/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6504 - accuracy: 0.6649 - val_loss: 0.6662 - val_accuracy: 0.6458\n",
      "Epoch 34/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6476 - accuracy: 0.6701 - val_loss: 0.6632 - val_accuracy: 0.6458\n",
      "Epoch 35/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6448 - accuracy: 0.6771 - val_loss: 0.6604 - val_accuracy: 0.6458\n",
      "Epoch 36/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6420 - accuracy: 0.6806 - val_loss: 0.6575 - val_accuracy: 0.6458\n",
      "Epoch 37/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6393 - accuracy: 0.6875 - val_loss: 0.6547 - val_accuracy: 0.6458\n",
      "Epoch 38/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6367 - accuracy: 0.6910 - val_loss: 0.6520 - val_accuracy: 0.6458\n",
      "Epoch 39/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6342 - accuracy: 0.6910 - val_loss: 0.6494 - val_accuracy: 0.6458\n",
      "Epoch 40/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6317 - accuracy: 0.6910 - val_loss: 0.6468 - val_accuracy: 0.6458\n",
      "Epoch 41/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6293 - accuracy: 0.6910 - val_loss: 0.6442 - val_accuracy: 0.6458\n",
      "Epoch 42/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6269 - accuracy: 0.6927 - val_loss: 0.6417 - val_accuracy: 0.6458\n",
      "Epoch 43/200\n",
      "18/18 [==============================] - ETA: 0s - loss: 0.6368 - accuracy: 0.56 - 0s 1ms/step - loss: 0.6245 - accuracy: 0.6927 - val_loss: 0.6393 - val_accuracy: 0.6458\n",
      "Epoch 44/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6222 - accuracy: 0.6944 - val_loss: 0.6369 - val_accuracy: 0.6458\n",
      "Epoch 45/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6200 - accuracy: 0.6979 - val_loss: 0.6345 - val_accuracy: 0.6458\n",
      "Epoch 46/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6177 - accuracy: 0.7031 - val_loss: 0.6322 - val_accuracy: 0.6510\n",
      "Epoch 47/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6156 - accuracy: 0.7031 - val_loss: 0.6299 - val_accuracy: 0.6562\n",
      "Epoch 48/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6134 - accuracy: 0.7014 - val_loss: 0.6276 - val_accuracy: 0.6615\n",
      "Epoch 49/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6113 - accuracy: 0.7031 - val_loss: 0.6253 - val_accuracy: 0.6719\n",
      "Epoch 50/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6092 - accuracy: 0.7049 - val_loss: 0.6231 - val_accuracy: 0.6719\n",
      "Epoch 51/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6071 - accuracy: 0.7049 - val_loss: 0.6208 - val_accuracy: 0.6719\n",
      "Epoch 52/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6051 - accuracy: 0.7049 - val_loss: 0.6187 - val_accuracy: 0.6719\n",
      "Epoch 53/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6031 - accuracy: 0.7066 - val_loss: 0.6165 - val_accuracy: 0.6771\n",
      "Epoch 54/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.6011 - accuracy: 0.7101 - val_loss: 0.6144 - val_accuracy: 0.6771\n",
      "Epoch 55/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5992 - accuracy: 0.7118 - val_loss: 0.6123 - val_accuracy: 0.6823\n",
      "Epoch 56/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5973 - accuracy: 0.7118 - val_loss: 0.6103 - val_accuracy: 0.6875\n",
      "Epoch 57/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5954 - accuracy: 0.7153 - val_loss: 0.6082 - val_accuracy: 0.6927\n",
      "Epoch 58/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5935 - accuracy: 0.7118 - val_loss: 0.6062 - val_accuracy: 0.6927\n",
      "Epoch 59/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5916 - accuracy: 0.7170 - val_loss: 0.6043 - val_accuracy: 0.6927\n",
      "Epoch 60/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5898 - accuracy: 0.7170 - val_loss: 0.6024 - val_accuracy: 0.6875\n",
      "Epoch 61/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5880 - accuracy: 0.7170 - val_loss: 0.6005 - val_accuracy: 0.6979\n",
      "Epoch 62/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5862 - accuracy: 0.7153 - val_loss: 0.5986 - val_accuracy: 0.7083\n",
      "Epoch 63/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5845 - accuracy: 0.7153 - val_loss: 0.5968 - val_accuracy: 0.7135\n",
      "Epoch 64/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5828 - accuracy: 0.7153 - val_loss: 0.5951 - val_accuracy: 0.7135\n",
      "Epoch 65/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5811 - accuracy: 0.7153 - val_loss: 0.5933 - val_accuracy: 0.7135\n",
      "Epoch 66/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5794 - accuracy: 0.7135 - val_loss: 0.5916 - val_accuracy: 0.7135\n",
      "Epoch 67/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5778 - accuracy: 0.7135 - val_loss: 0.5899 - val_accuracy: 0.7135\n",
      "Epoch 68/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5762 - accuracy: 0.7135 - val_loss: 0.5882 - val_accuracy: 0.7135\n",
      "Epoch 69/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5746 - accuracy: 0.7135 - val_loss: 0.5866 - val_accuracy: 0.7135\n",
      "Epoch 70/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5730 - accuracy: 0.7118 - val_loss: 0.5850 - val_accuracy: 0.7135\n",
      "Epoch 71/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5715 - accuracy: 0.7135 - val_loss: 0.5834 - val_accuracy: 0.7135\n",
      "Epoch 72/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5700 - accuracy: 0.7135 - val_loss: 0.5818 - val_accuracy: 0.7135\n",
      "Epoch 73/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5685 - accuracy: 0.7170 - val_loss: 0.5803 - val_accuracy: 0.7135\n",
      "Epoch 74/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5670 - accuracy: 0.7170 - val_loss: 0.5789 - val_accuracy: 0.7135\n",
      "Epoch 75/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5655 - accuracy: 0.7170 - val_loss: 0.5774 - val_accuracy: 0.7240\n",
      "Epoch 76/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.7170 - val_loss: 0.5760 - val_accuracy: 0.7240\n",
      "Epoch 77/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5627 - accuracy: 0.7153 - val_loss: 0.5746 - val_accuracy: 0.7240\n",
      "Epoch 78/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.5613 - accuracy: 0.7170 - val_loss: 0.5732 - val_accuracy: 0.7240\n",
      "Epoch 79/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5599 - accuracy: 0.7170 - val_loss: 0.5718 - val_accuracy: 0.7240\n",
      "Epoch 80/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5586 - accuracy: 0.7170 - val_loss: 0.5704 - val_accuracy: 0.7240\n",
      "Epoch 81/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5572 - accuracy: 0.7205 - val_loss: 0.5691 - val_accuracy: 0.7240\n",
      "Epoch 82/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5559 - accuracy: 0.7222 - val_loss: 0.5678 - val_accuracy: 0.7292\n",
      "Epoch 83/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5545 - accuracy: 0.7222 - val_loss: 0.5665 - val_accuracy: 0.7344\n",
      "Epoch 84/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5532 - accuracy: 0.7257 - val_loss: 0.5652 - val_accuracy: 0.7344\n",
      "Epoch 85/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5519 - accuracy: 0.7257 - val_loss: 0.5639 - val_accuracy: 0.7344\n",
      "Epoch 86/200\n",
      "18/18 [==============================] - 0s 998us/step - loss: 0.5506 - accuracy: 0.7274 - val_loss: 0.5627 - val_accuracy: 0.7344\n",
      "Epoch 87/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5493 - accuracy: 0.7292 - val_loss: 0.5615 - val_accuracy: 0.7292\n",
      "Epoch 88/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5480 - accuracy: 0.7326 - val_loss: 0.5602 - val_accuracy: 0.7292\n",
      "Epoch 89/200\n",
      "18/18 [==============================] - 0s 939us/step - loss: 0.5467 - accuracy: 0.7309 - val_loss: 0.5591 - val_accuracy: 0.7292\n",
      "Epoch 90/200\n",
      "18/18 [==============================] - 0s 997us/step - loss: 0.5455 - accuracy: 0.7309 - val_loss: 0.5579 - val_accuracy: 0.7396\n",
      "Epoch 91/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5442 - accuracy: 0.7344 - val_loss: 0.5567 - val_accuracy: 0.7448\n",
      "Epoch 92/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5430 - accuracy: 0.7378 - val_loss: 0.5556 - val_accuracy: 0.7500\n",
      "Epoch 93/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5418 - accuracy: 0.7361 - val_loss: 0.5545 - val_accuracy: 0.7500\n",
      "Epoch 94/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5406 - accuracy: 0.7361 - val_loss: 0.5534 - val_accuracy: 0.7448\n",
      "Epoch 95/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5395 - accuracy: 0.7361 - val_loss: 0.5524 - val_accuracy: 0.7448\n",
      "Epoch 96/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5383 - accuracy: 0.7361 - val_loss: 0.5513 - val_accuracy: 0.7500\n",
      "Epoch 97/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5371 - accuracy: 0.7378 - val_loss: 0.5503 - val_accuracy: 0.7500\n",
      "Epoch 98/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5360 - accuracy: 0.7361 - val_loss: 0.5493 - val_accuracy: 0.7448\n",
      "Epoch 99/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5349 - accuracy: 0.7361 - val_loss: 0.5482 - val_accuracy: 0.7448\n",
      "Epoch 100/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5337 - accuracy: 0.7361 - val_loss: 0.5473 - val_accuracy: 0.7448\n",
      "Epoch 101/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5326 - accuracy: 0.7344 - val_loss: 0.5463 - val_accuracy: 0.7448\n",
      "Epoch 102/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5315 - accuracy: 0.7292 - val_loss: 0.5453 - val_accuracy: 0.7396\n",
      "Epoch 103/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5305 - accuracy: 0.7309 - val_loss: 0.5444 - val_accuracy: 0.7396\n",
      "Epoch 104/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5294 - accuracy: 0.7326 - val_loss: 0.5435 - val_accuracy: 0.7396\n",
      "Epoch 105/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5284 - accuracy: 0.7344 - val_loss: 0.5425 - val_accuracy: 0.7396\n",
      "Epoch 106/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5273 - accuracy: 0.7361 - val_loss: 0.5417 - val_accuracy: 0.7396\n",
      "Epoch 107/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5263 - accuracy: 0.7361 - val_loss: 0.5408 - val_accuracy: 0.7448\n",
      "Epoch 108/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5253 - accuracy: 0.7378 - val_loss: 0.5399 - val_accuracy: 0.7448\n",
      "Epoch 109/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5244 - accuracy: 0.7396 - val_loss: 0.5391 - val_accuracy: 0.7448\n",
      "Epoch 110/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5234 - accuracy: 0.7396 - val_loss: 0.5382 - val_accuracy: 0.7552\n",
      "Epoch 111/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5224 - accuracy: 0.7413 - val_loss: 0.5374 - val_accuracy: 0.7552\n",
      "Epoch 112/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5215 - accuracy: 0.7413 - val_loss: 0.5366 - val_accuracy: 0.7552\n",
      "Epoch 113/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5205 - accuracy: 0.7413 - val_loss: 0.5359 - val_accuracy: 0.7552\n",
      "Epoch 114/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5196 - accuracy: 0.7413 - val_loss: 0.5351 - val_accuracy: 0.7552\n",
      "Epoch 115/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5187 - accuracy: 0.7378 - val_loss: 0.5343 - val_accuracy: 0.7552\n",
      "Epoch 116/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5178 - accuracy: 0.7378 - val_loss: 0.5336 - val_accuracy: 0.7552\n",
      "Epoch 117/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5170 - accuracy: 0.7344 - val_loss: 0.5329 - val_accuracy: 0.7552\n",
      "Epoch 118/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5161 - accuracy: 0.7344 - val_loss: 0.5322 - val_accuracy: 0.7552\n",
      "Epoch 119/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5153 - accuracy: 0.7361 - val_loss: 0.5315 - val_accuracy: 0.7552\n",
      "Epoch 120/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5144 - accuracy: 0.7344 - val_loss: 0.5308 - val_accuracy: 0.7552\n",
      "Epoch 121/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5136 - accuracy: 0.7344 - val_loss: 0.5301 - val_accuracy: 0.7552\n",
      "Epoch 122/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5128 - accuracy: 0.7326 - val_loss: 0.5294 - val_accuracy: 0.7500\n",
      "Epoch 123/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5120 - accuracy: 0.7361 - val_loss: 0.5287 - val_accuracy: 0.7500\n",
      "Epoch 124/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5112 - accuracy: 0.7361 - val_loss: 0.5281 - val_accuracy: 0.7448\n",
      "Epoch 125/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5104 - accuracy: 0.7344 - val_loss: 0.5275 - val_accuracy: 0.7448\n",
      "Epoch 126/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5097 - accuracy: 0.7361 - val_loss: 0.5269 - val_accuracy: 0.7448\n",
      "Epoch 127/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5089 - accuracy: 0.7378 - val_loss: 0.5263 - val_accuracy: 0.7448\n",
      "Epoch 128/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5082 - accuracy: 0.7378 - val_loss: 0.5257 - val_accuracy: 0.7448\n",
      "Epoch 129/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5075 - accuracy: 0.7378 - val_loss: 0.5251 - val_accuracy: 0.7448\n",
      "Epoch 130/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5068 - accuracy: 0.7396 - val_loss: 0.5246 - val_accuracy: 0.7448\n",
      "Epoch 131/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5061 - accuracy: 0.7396 - val_loss: 0.5240 - val_accuracy: 0.7448\n",
      "Epoch 132/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5054 - accuracy: 0.7396 - val_loss: 0.5235 - val_accuracy: 0.7500\n",
      "Epoch 133/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5047 - accuracy: 0.7396 - val_loss: 0.5229 - val_accuracy: 0.7500\n",
      "Epoch 134/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5041 - accuracy: 0.7465 - val_loss: 0.5224 - val_accuracy: 0.7500\n",
      "Epoch 135/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5034 - accuracy: 0.7465 - val_loss: 0.5219 - val_accuracy: 0.7500\n",
      "Epoch 136/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7465 - val_loss: 0.5214 - val_accuracy: 0.7604\n",
      "Epoch 137/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5021 - accuracy: 0.7465 - val_loss: 0.5209 - val_accuracy: 0.7604\n",
      "Epoch 138/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5015 - accuracy: 0.7465 - val_loss: 0.5205 - val_accuracy: 0.7604\n",
      "Epoch 139/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5009 - accuracy: 0.7448 - val_loss: 0.5200 - val_accuracy: 0.7552\n",
      "Epoch 140/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.5003 - accuracy: 0.7483 - val_loss: 0.5196 - val_accuracy: 0.7552\n",
      "Epoch 141/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4997 - accuracy: 0.7448 - val_loss: 0.5191 - val_accuracy: 0.7552\n",
      "Epoch 142/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4991 - accuracy: 0.7448 - val_loss: 0.5187 - val_accuracy: 0.7552\n",
      "Epoch 143/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4985 - accuracy: 0.7448 - val_loss: 0.5182 - val_accuracy: 0.7552\n",
      "Epoch 144/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4979 - accuracy: 0.7448 - val_loss: 0.5178 - val_accuracy: 0.7552\n",
      "Epoch 145/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4974 - accuracy: 0.7483 - val_loss: 0.5175 - val_accuracy: 0.7552\n",
      "Epoch 146/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4968 - accuracy: 0.7483 - val_loss: 0.5171 - val_accuracy: 0.7552\n",
      "Epoch 147/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4962 - accuracy: 0.7465 - val_loss: 0.5167 - val_accuracy: 0.7552\n",
      "Epoch 148/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4957 - accuracy: 0.7465 - val_loss: 0.5163 - val_accuracy: 0.7500\n",
      "Epoch 149/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4952 - accuracy: 0.7465 - val_loss: 0.5160 - val_accuracy: 0.7500\n",
      "Epoch 150/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4946 - accuracy: 0.7413 - val_loss: 0.5156 - val_accuracy: 0.7500\n",
      "Epoch 151/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4941 - accuracy: 0.7396 - val_loss: 0.5153 - val_accuracy: 0.7500\n",
      "Epoch 152/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4936 - accuracy: 0.7396 - val_loss: 0.5150 - val_accuracy: 0.7448\n",
      "Epoch 153/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4930 - accuracy: 0.7413 - val_loss: 0.5146 - val_accuracy: 0.7448\n",
      "Epoch 154/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4925 - accuracy: 0.7413 - val_loss: 0.5143 - val_accuracy: 0.7448\n",
      "Epoch 155/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4920 - accuracy: 0.7413 - val_loss: 0.5140 - val_accuracy: 0.7448\n",
      "Epoch 156/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4915 - accuracy: 0.7413 - val_loss: 0.5137 - val_accuracy: 0.7448\n",
      "Epoch 157/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4910 - accuracy: 0.7431 - val_loss: 0.5134 - val_accuracy: 0.7448\n",
      "Epoch 158/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4905 - accuracy: 0.7431 - val_loss: 0.5132 - val_accuracy: 0.7448\n",
      "Epoch 159/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4900 - accuracy: 0.7431 - val_loss: 0.5129 - val_accuracy: 0.7448\n",
      "Epoch 160/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4896 - accuracy: 0.7448 - val_loss: 0.5127 - val_accuracy: 0.7448\n",
      "Epoch 161/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4891 - accuracy: 0.7465 - val_loss: 0.5124 - val_accuracy: 0.7448\n",
      "Epoch 162/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4887 - accuracy: 0.7465 - val_loss: 0.5122 - val_accuracy: 0.7448\n",
      "Epoch 163/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4882 - accuracy: 0.7465 - val_loss: 0.5119 - val_accuracy: 0.7448\n",
      "Epoch 164/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4878 - accuracy: 0.7465 - val_loss: 0.5117 - val_accuracy: 0.7448\n",
      "Epoch 165/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4874 - accuracy: 0.7448 - val_loss: 0.5115 - val_accuracy: 0.7448\n",
      "Epoch 166/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4869 - accuracy: 0.7448 - val_loss: 0.5113 - val_accuracy: 0.7448\n",
      "Epoch 167/200\n",
      "18/18 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7448 - val_loss: 0.5111 - val_accuracy: 0.7448\n",
      "Epoch 168/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4861 - accuracy: 0.7431 - val_loss: 0.5109 - val_accuracy: 0.7448\n",
      "Epoch 169/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4857 - accuracy: 0.7465 - val_loss: 0.5107 - val_accuracy: 0.7448\n",
      "Epoch 170/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4853 - accuracy: 0.7465 - val_loss: 0.5105 - val_accuracy: 0.7448\n",
      "Epoch 171/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4849 - accuracy: 0.7483 - val_loss: 0.5103 - val_accuracy: 0.7448\n",
      "Epoch 172/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4845 - accuracy: 0.7483 - val_loss: 0.5101 - val_accuracy: 0.7448\n",
      "Epoch 173/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4841 - accuracy: 0.7500 - val_loss: 0.5099 - val_accuracy: 0.7448\n",
      "Epoch 174/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4837 - accuracy: 0.7517 - val_loss: 0.5098 - val_accuracy: 0.7448\n",
      "Epoch 175/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4833 - accuracy: 0.7517 - val_loss: 0.5096 - val_accuracy: 0.7448\n",
      "Epoch 176/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4829 - accuracy: 0.7535 - val_loss: 0.5094 - val_accuracy: 0.7448\n",
      "Epoch 177/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4825 - accuracy: 0.7535 - val_loss: 0.5092 - val_accuracy: 0.7448\n",
      "Epoch 178/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4822 - accuracy: 0.7535 - val_loss: 0.5091 - val_accuracy: 0.7448\n",
      "Epoch 179/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4818 - accuracy: 0.7535 - val_loss: 0.5089 - val_accuracy: 0.7500\n",
      "Epoch 180/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4814 - accuracy: 0.7535 - val_loss: 0.5088 - val_accuracy: 0.7500\n",
      "Epoch 181/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4811 - accuracy: 0.7535 - val_loss: 0.5086 - val_accuracy: 0.7500\n",
      "Epoch 182/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4807 - accuracy: 0.7535 - val_loss: 0.5085 - val_accuracy: 0.7500\n",
      "Epoch 183/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4804 - accuracy: 0.7535 - val_loss: 0.5084 - val_accuracy: 0.7500\n",
      "Epoch 184/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4801 - accuracy: 0.7535 - val_loss: 0.5082 - val_accuracy: 0.7448\n",
      "Epoch 185/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4797 - accuracy: 0.7535 - val_loss: 0.5081 - val_accuracy: 0.7448\n",
      "Epoch 186/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4794 - accuracy: 0.7535 - val_loss: 0.5080 - val_accuracy: 0.7500\n",
      "Epoch 187/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4790 - accuracy: 0.7535 - val_loss: 0.5079 - val_accuracy: 0.7500\n",
      "Epoch 188/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4787 - accuracy: 0.7535 - val_loss: 0.5077 - val_accuracy: 0.7500\n",
      "Epoch 189/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4784 - accuracy: 0.7552 - val_loss: 0.5076 - val_accuracy: 0.7500\n",
      "Epoch 190/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4781 - accuracy: 0.7535 - val_loss: 0.5075 - val_accuracy: 0.7500\n",
      "Epoch 191/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4778 - accuracy: 0.7535 - val_loss: 0.5074 - val_accuracy: 0.7500\n",
      "Epoch 192/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4775 - accuracy: 0.7535 - val_loss: 0.5073 - val_accuracy: 0.7500\n",
      "Epoch 193/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4772 - accuracy: 0.7535 - val_loss: 0.5072 - val_accuracy: 0.7500\n",
      "Epoch 194/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4769 - accuracy: 0.7535 - val_loss: 0.5070 - val_accuracy: 0.7500\n",
      "Epoch 195/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4766 - accuracy: 0.7535 - val_loss: 0.5069 - val_accuracy: 0.7500\n",
      "Epoch 196/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4763 - accuracy: 0.7535 - val_loss: 0.5068 - val_accuracy: 0.7500\n",
      "Epoch 197/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4760 - accuracy: 0.7535 - val_loss: 0.5067 - val_accuracy: 0.7500\n",
      "Epoch 198/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4758 - accuracy: 0.7535 - val_loss: 0.5066 - val_accuracy: 0.7500\n",
      "Epoch 199/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4755 - accuracy: 0.7535 - val_loss: 0.5065 - val_accuracy: 0.7500\n",
      "Epoch 200/200\n",
      "18/18 [==============================] - 0s 1ms/step - loss: 0.4752 - accuracy: 0.7535 - val_loss: 0.5064 - val_accuracy: 0.7500\n"
     ]
    }
   ],
   "source": [
    "# Fit(Train) the Model\n",
    "\n",
    "# Compile the model with Optimizer, Loss Function and Metrics\n",
    "# Roc-Auc is not available in Keras as an off the shelf metric yet, so we will skip it here.\n",
    "\n",
    "model_1.compile(SGD(lr = .003), \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "run_hist_1 = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=200)\n",
    "# the fit function returns the run history. \n",
    "# It is very convenient, as it contains information about the model fit, iterations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Like we did for the Random Forest, we generate two kinds of predictions\n",
    "#  One is a hard decision, the other is a probabilitistic score.\n",
    "\n",
    "y_pred_class_nn_1 = model_1.predict_classes(X_test_norm)\n",
    "y_pred_prob_nn_1 = model_1.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's check out the outputs to get a feel for how keras apis work.\n",
    "y_pred_class_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_prob_nn_1[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model performance and plot the roc curve\n",
    "print('accuracy is {:.3f}'.format(accuracy_score(y_test,y_pred_class_nn_1)))\n",
    "print('roc-auc is {:.3f}'.format(roc_auc_score(y_test,y_pred_prob_nn_1)))\n",
    "\n",
    "plot_roc(y_test, y_pred_prob_nn_1, 'NN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be some variation in exact numbers due to randomness, but you should get results in the same ballpark as the Random Forest - between 75% and 85% accuracy, between .8 and .9 for AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the `run_hist_1` object that was created, specifically its `history` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hist_1.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training loss and the validation loss over the different epochs and see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss\")\n",
    "ax.plot(run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss\")\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the losses are still going down on both the training set and the validation set.  This suggests that the model might benefit from further training.  Let's train the model a little more and see what happens. Note that it will pick up from where it left off. Train for 1000 more epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Note that when we call \"fit\" again, it picks up where it left off\n",
    "run_hist_1b = model_1.fit(X_train_norm, y_train, validation_data=(X_test_norm, y_test), epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(run_hist_1.history[\"loss\"])\n",
    "m = len(run_hist_1b.history['loss'])\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"loss\"],'r', marker='.', label=\"Train Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"loss\"], 'hotpink', marker='.', label=\"Train Loss - Run 2\")\n",
    "\n",
    "ax.plot(range(n), run_hist_1.history[\"val_loss\"],'b', marker='.', label=\"Validation Loss - Run 1\")\n",
    "ax.plot(range(n, n+m), run_hist_1b.history[\"val_loss\"], 'LightSkyBlue', marker='.',  label=\"Validation Loss - Run 2\")\n",
    "\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this graph begins where the other left off.  While the training loss is still going down, it looks like the validation loss has stabilized (or even gotten worse!).  This suggests that our network will not benefit from further training.  What is the appropriate number of epochs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "Now it's your turn.  Do the following in the cells below:\n",
    "- Build a model with two hidden layers, each with 6 nodes\n",
    "- Use the \"relu\" activation function for the hidden layers, and \"sigmoid\" for the final layer\n",
    "- Use a learning rate of .003 and train for 1500 epochs\n",
    "- Graph the trajectory of the loss functions, accuracy on both train and test set\n",
    "- Plot the roc curve for the predictions\n",
    "\n",
    "Experiment with different learning rates, numbers of epochs, and network structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
